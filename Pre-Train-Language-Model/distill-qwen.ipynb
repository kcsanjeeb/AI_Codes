{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Checking GPU Availability in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code checks if a CUDA-compatible GPU is available, prints the number of GPUs detected, and displays the name of the first GPU (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "\n",
    "ai_mathematical_olympiad_progress_prize_2_path = kagglehub.competition_download('ai-mathematical-olympiad-progress-prize-2')\n",
    "#mpware_vllm_0_7_1_path = kagglehub.utility_script_install('mpware/vllm-0-7-1')\n",
    "deepseek_ai_deepseek_r1_transformers_deepseek_r1_distill_qwen_1_5b_2_path = kagglehub.model_download('deepseek-ai/deepseek-r1/Transformers/deepseek-r1-distill-qwen-1.5b/2')\n",
    "deepseek_ai_deepseek_r1_transformers_deepseek_r1_distill_qwen_7b_2_path = kagglehub.model_download('deepseek-ai/deepseek-r1/Transformers/deepseek-r1-distill-qwen-7b/2')\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle  # Create config directory\n",
    "!echo '{\"username\":\"kcsanjeeb\",\"key\":\"47c837ff8defc1d44aa31298fe673264\"}' > ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json  # Restrict permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle kernels output mpware/vllm-0-7-1 -p /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "import kaggle_evaluation.aimo_2_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TRITON_PTXAS_PATH\"] = \"/usr/local/cuda/bin/ptxas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the Cutoff times as per the competiton rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "cutoff_time = start_time + (4 * 60 + 45) * 60\n",
    "cutoff_times = [int(x) for x in np.linspace(cutoff_time, start_time + 60 * 60, 50 + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]   = \"0,1,2,3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv\")\n",
    "df_test=pd.read_csv(\"/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv\")\n",
    "sample_submission=pd.read_csv(\"/kaggle/input/ai-mathematical-olympiad-progress-prize-2/sample_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=df[\"problem\"][0]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Define the model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model path\n",
    "model_path = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-1.5b/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_SEQS = 128\n",
    "MAX_MODEL_LEN = 8192 * 3 // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Loading the LLM Model with vLLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code initializes an LLM model using vLLM with specific configurations, such as setting the data type to float16, defining the maximum number of sequences, context length, GPU memory utilization, and enabling remote code trust for downloading the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "# Load the model\n",
    "llm = LLM(model_path,dtype=\"float16\",max_num_seqs=MAX_NUM_SEQS,   # Maximum number of sequences per iteration. Default is 256\n",
    "    max_model_len=MAX_MODEL_LEN, # Model context length\n",
    "    trust_remote_code=True,      # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n",
    "    tensor_parallel_size=1,      # The number of GPUs to use for distributed execution with tensor parallelism\n",
    "    gpu_memory_utilization=0.95, # The ratio (between 0 and 1) of GPU memory to reserve for the model\n",
    "    seed=2024,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prompt Formatting for Math Problem Solving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function structures the input prompt to enforce strict formatting rules for an LLM solving math problems. It ensures that the model outputs only the final numeric answer inside \\boxed{} without any explanations or intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(problem_text):\n",
    "    return (\n",
    "        \"Solve the following math problem exactly.\\n\"\n",
    "        \"Return ONLY the final numeric answer inside \\\\boxed{}.\\n\"\n",
    "        \"THIS IS NOT A TRICK QUESTION. DON'T OVERTHINK\"\n",
    "        \"DO NOT include explanations, reasoning, or intermediate steps.\\n\"\n",
    "        \"DO NOT repeat or rephrase the question.\\n\"\n",
    "        \"DO NOT output anything except \\\\boxed{ANSWER}.\\n\"\n",
    "        \"Provide the final numeric answer inside \\\\boxed{} only \\n\\n\"\n",
    "        \"Problem: \" + problem_text\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " max_tokens = MAX_MODEL_LEN\n",
    "if time.time() > cutoff_times[-1]:\n",
    "    print(\"Speedrun\")\n",
    "    max_tokens = 2 * MAX_MODEL_LEN // 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sampling Parameters for Math Problem Solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=1.0,      # Forces deterministic, precise answers\n",
    "    top_p=1.0,           # Consider all likely tokens (not limiting)\n",
    "    top_k=-1,            # No top-k restriction\n",
    "    max_tokens=max_tokens,       # Small max tokens to prevent long reasoning\n",
    "    repetition_penalty=1.1,  # Discourage repeated phrases\n",
    "    ignore_eos=False,    # Let generation stop naturally\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This Will Generate the full Response Using Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def solve_math_problem_full_response(problem_text):\n",
    "#     # Tokenize input\n",
    "#     inputs = tokenizer(problem_text, return_tensors=\"pt\").to(\"cuda\") \n",
    "    \n",
    "#     # Generate response\n",
    "#     with torch.no_grad():\n",
    "#         output = model.generate(**inputs, max_new_tokens=700,  # Increased max_new_tokens\n",
    "#                         eos_token_id=tokenizer.eos_token_id,\n",
    "#                         temperature=0.9,  # Higher temperature\n",
    "#                         top_p=0.85,       # Adjusted top_p\n",
    "#                         num_beams=6,       # More beams\n",
    "#                         repetition_penalty=1.2) # Penalize repetitions     \n",
    "    \n",
    "#     # Decode the response\n",
    "#     response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "#     return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response Using Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"what is 10+5\"\n",
    "# answer = solve_math_problem_full_response(question)\n",
    "# print(\"Generated Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.  Generating a Math Solution Using vLLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formats the problem prompt to enforce strict answer formatting inside \\boxed{}.\n",
    "Uses vLLM to generate responses based on predefined sampling parameters.\n",
    "Prints the generated output while ensuring no intermediate steps or explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_text = \"What is 10 * 5\"\n",
    "formatted_prompt = format_prompt(problem_text)\n",
    "# Generate response\n",
    "outputs = llm.generate(formatted_prompt,sampling_params)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, \\nGenerated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. To calculate the Modulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_answer(result):\n",
    "    try:\n",
    "        result = int(result)  # Convert to integer\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid input: {result} is not a number\")\n",
    "\n",
    "    # Apply modulo 1000 only if result is greater than 999 or negative\n",
    "    if result > 999 or result < 0:\n",
    "        final_answer = result % 1000\n",
    "        # Ensure positive modulo result\n",
    "        final_answer = final_answer if final_answer >= 0 else final_answer + 1000\n",
    "    else:\n",
    "        final_answer = result\n",
    "\n",
    "    return final_answer  # Returning as an integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_final_answer(1023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Extracting Final Answer from boxed response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_boxed_text(text: str) -> str:\n",
    "    pattern = r'boxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    for match in matches[::-1]:\n",
    "        if match != \"\":\n",
    "            return match\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer= extract_boxed_text(generated_text)\n",
    "print(\"Final Generated Answer:\",final_answer )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_final_answer(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Only Final Boxed Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def extract_boxed_text(text: str) -> str:\n",
    "#     pattern = r'boxed{(.*?)}'\n",
    "#     matches = re.findall(pattern, text)\n",
    "#     if not matches:\n",
    "#         return \"\"\n",
    "#     for match in matches[::-1]:\n",
    "#         if match != \"\":\n",
    "#             return match\n",
    "#     return \"\"\n",
    "\n",
    "# def solve_math_problem(problem_text):\n",
    "#     formatted_prompt = (\n",
    "#     \"Solve the following math problem exactly. Do not approximate. \"\n",
    "#     \"Return ONLY the correct final numeric answer inside \\\\boxed{}. \"\n",
    "#     \"Do NOT include explanations, reasoning, or intermediate steps. \"\n",
    "#     \"If the answer is negative, still use \\\\boxed{}. \"\n",
    "#     \"Ensure the answer is 100% correct before returning. \"\n",
    "#     \"Problem: \" + problem_text\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "#     inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         output = model.generate(**inputs, max_new_tokens=700,  \n",
    "#                         eos_token_id=tokenizer.eos_token_id,\n",
    "#                         temperature=0.9, \n",
    "#                         top_p=0.85,      \n",
    "#                         num_beams=6,     \n",
    "#                         repetition_penalty=1.2)\n",
    "#     response = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "#     return extract_boxed_text(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"what is 10+5\"\n",
    "# answer = solve_math_problem(question)\n",
    "# print(\"Generated Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.  Method that combines prompt formatting, model generation, and extracting the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def solve_math_problem_vllm(llm, problem_text, sampling_params):\n",
    "    \"\"\"\n",
    "    Generates a response using the LLM, extracts the final boxed answer, and returns it.\n",
    "    \n",
    "    Args:\n",
    "        llm: The language model instance.\n",
    "        problem_text (str): The math problem to solve.\n",
    "        sampling_params: Parameters for the model generation.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted final numeric answer from \\boxed{}.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the prompt\n",
    "    formatted_prompt = format_prompt(problem_text)\n",
    "\n",
    "    # Generate response from the model\n",
    "    outputs = llm.generate(formatted_prompt, sampling_params)\n",
    "\n",
    "    # Extract generated text\n",
    "    generated_text = outputs[0].outputs[0].text if outputs else \"\"\n",
    "\n",
    "    # Extract the boxed answer\n",
    "    final_answer = extract_boxed_text(generated_text)\n",
    "    # Modulo \n",
    "    prediction_modulo=compute_final_answer(final_answer)  #Take the Modulo \n",
    "\n",
    "    # Print results\n",
    "    print(f\"Prompt: {formatted_prompt!r}\\n\")\n",
    "    print(f\"Generated text: {generated_text!r}\\n\")\n",
    "    print(f\"Final Generated Answer: {prediction_modulo}\")\n",
    "    return prediction_modulo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"problem\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_text = df_test[\"problem\"][0]\n",
    "final_answer = solve_math_problem_vllm(llm, problem_text, sampling_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to process all rows and generate predictions using Pandas\n",
    "def generate_submission(df_test):\n",
    "    \"\"\"Generate predictions for all rows in df_test and save to submission.csv.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for _, row in df_test.iterrows():  # Iterate through Pandas DataFrame rows\n",
    "        id_ = row[\"id\"]\n",
    "        question = row[\"problem\"]\n",
    "\n",
    "        print(f\"Processing ID: {id_}, Question: {question}\")\n",
    "\n",
    "        # Generate prediction using LLM\n",
    "        prediction = solve_math_problem_vllm(llm, question, sampling_params)\n",
    "\n",
    "        # Append result to list\n",
    "        results.append({\"id\": id_, \"answer\": prediction})\n",
    "\n",
    "    # Convert results to Pandas DataFrame\n",
    "    submission_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save as CSV\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission saved as submission.csv\")\n",
    "\n",
    "# Call the function\n",
    "# generate_submission(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submission(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 999, inclusive.\n",
    "# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    # Unpack values\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "    \n",
    "    question = question.item(0)\n",
    "    print(question)\n",
    "    # Generate prediction using the model\n",
    "    prediction = solve_math_problem_vllm(llm, question, sampling_params)  # Get boxed answer\n",
    "        \n",
    "    print(\"------\\n\\n\\n\")\n",
    "\n",
    "    print(\"Final Predicted Answer is\",prediction)\n",
    "         \n",
    "    return pl.DataFrame({'id': [id_], 'answer': prediction})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            \"/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv\",\n",
    "            # \"reference.csv\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
