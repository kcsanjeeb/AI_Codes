{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download Modal and Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707b97ba6c984c23aa4ff5e85a3d2363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://www.kaggle.com/static/images/site-logo.png\\nalt=\\'Kaggleâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kagglehub\n",
    "kagglehub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_mathematical_olympiad_progress_prize_2_path = kagglehub.competition_download('ai-mathematical-olympiad-progress-prize-2')\n",
    "#mpware_vllm_0_7_1_path = kagglehub.utility_script_install('mpware/vllm-0-7-1')\n",
    "deepseek_ai_deepseek_r1_transformers_deepseek_r1_distill_qwen_1_5b_2_path = kagglehub.model_download('deepseek-ai/deepseek-r1/Transformers/deepseek-r1-distill-qwen-1.5b/2')\n",
    "deepseek_ai_deepseek_r1_transformers_deepseek_r1_distill_qwen_7b_2_path = kagglehub.model_download('deepseek-ai/deepseek-r1/Transformers/deepseek-r1-distill-qwen-7b/2')\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mcompetitions\u001b[m\u001b[m \u001b[34mmodels\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls ~/.cache/kagglehub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached data and models:\n",
      "Competition data: /Users/sanjeeb/.cache/kagglehub/competitions/ai-mathematical-olympiad-progress-prize-2\n",
      "1.5B model: /Users/sanjeeb/.cache/kagglehub/models/deepseek-ai/deepseek-r1/Transformers/deepseek-r1-distill-qwen-1.5b/2\n",
      "7B model: /Users/sanjeeb/.cache/kagglehub/models/deepseek-ai/deepseek-r1/Transformers/deepseek-r1-distill-qwen-7b/2\n"
     ]
    }
   ],
   "source": [
    "# Define paths manually using your existing cached locations\n",
    "ai_mathematical_olympiad_progress_prize_2_path = \"/Users/sanjeeb/.cache/kagglehub/competitions/ai-mathematical-olympiad-progress-prize-2\"\n",
    "deepseek_ai_deepseek_r1_transformers_deepseek_r1_distill_qwen_1_5b_2_path = \"/Users/sanjeeb/.cache/kagglehub/models/deepseek-ai/deepseek-r1/Transformers/deepseek-r1-distill-qwen-1.5b/2\"\n",
    "deepseek_ai_deepseek_r1_transformers_deepseek_r1_distill_qwen_7b_2_path = \"/Users/sanjeeb/.cache/kagglehub/models/deepseek-ai/deepseek-r1/Transformers/deepseek-r1-distill-qwen-7b/2\"\n",
    "\n",
    "print(\"Using cached data and models:\")\n",
    "print(\"Competition data:\", ai_mathematical_olympiad_progress_prize_2_path)\n",
    "print(\"1.5B model:\", deepseek_ai_deepseek_r1_transformers_deepseek_r1_distill_qwen_1_5b_2_path)\n",
    "print(\"7B model:\", deepseek_ai_deepseek_r1_transformers_deepseek_r1_distill_qwen_7b_2_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "#import kaggle_evaluation.aimo_2_inference_server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the Cutoff times as per the competiton rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "cutoff_time = start_time + (4 * 60 + 45) * 60\n",
    "cutoff_times = [int(x) for x in np.linspace(cutoff_time, start_time + 60 * 60, 50 + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{ai_mathematical_olympiad_progress_prize_2_path}/reference.csv\")\n",
    "df_test = pd.read_csv(f\"{ai_mathematical_olympiad_progress_prize_2_path}/test.csv\")\n",
    "sample_submission = pd.read_csv(f\"{ai_mathematical_olympiad_progress_prize_2_path}/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>057f8a</td>\n",
       "      <td>Three airline companies operate flights from D...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192e23</td>\n",
       "      <td>Fred and George take part in a tennis tourname...</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1acac0</td>\n",
       "      <td>Triangle $ABC$ has side length $AB = 120$ and ...</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1fce4b</td>\n",
       "      <td>Find the three-digit number $n$ such that writ...</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>349493</td>\n",
       "      <td>We call a sequence $a_1, a_2, \\ldots$ of non-n...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                            problem  answer\n",
       "0  057f8a  Three airline companies operate flights from D...      79\n",
       "1  192e23  Fred and George take part in a tennis tourname...     250\n",
       "2  1acac0  Triangle $ABC$ has side length $AB = 120$ and ...     180\n",
       "3  1fce4b  Find the three-digit number $n$ such that writ...     143\n",
       "4  349493  We call a sequence $a_1, a_2, \\ldots$ of non-n...       3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Three airline companies operate flights from Dodola island. Each company has a different schedule of departures. The first company departs every 100 days, the second every 120 days and the third every 150 days. What is the greatest positive integer $d$ for which it is true that there will be $d$ consecutive days without a flight from Dodola island, regardless of the departure times of the various airlines?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=df[\"problem\"][0]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>problem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000aaa</td>\n",
       "      <td>What is $1-1$?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111bbb</td>\n",
       "      <td>What is $0\\times10$?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222ccc</td>\n",
       "      <td>Solve $4+x=4$ for $x$.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                 problem\n",
       "0  000aaa          What is $1-1$?\n",
       "1  111bbb    What is $0\\times10$?\n",
       "2  222ccc  Solve $4+x=4$ for $x$."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  Define the model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model path\n",
    "model_path = deepseek_ai_deepseek_r1_transformers_deepseek_r1_distill_qwen_1_5b_2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# # Load tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_SEQS = 128\n",
    "MAX_MODEL_LEN = 8192 * 3 // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing VLLM in MacOS\n",
    "* git clone https://github.com/vllm-project/vllm.git\n",
    "* cd vllm\n",
    "Direct installation - might work on some systems without additional flags\n",
    "* pip install -e .\n",
    "If direct installation fails with CUDA errors, use these environment variables\n",
    "to bypass CUDA requirements (optional but often necessary):\n",
    "* export VLLM_TARGET_DEVICE=cpu\n",
    "* export VLLM_BUILD_WITH_CUDA=0\n",
    "* pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if MPS (Metal) is available\n",
    "print(torch.backends.mps.is_available())  # Should return True on M1/M2 Macs\n",
    "print(torch.backends.mps.is_built())      # True if PyTorch supports MPS\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Use Apple Metal GPU\n",
    "    print(f\"Using device: {device}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU acceleration available (MPS not supported)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/Users/sanjeeb/Desktop/Harbin Institute of Technology/Artificial Intelligence/AI_Codes/Pre-Train-Language-Model/convert.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Convert the model (run from inside llama.cpp)\n",
    "! python convert.py --input /Users/sanjeeb/.cache/kagglehub/models/deepseek-ai/deepseek-r1/Transformers/deepseek-r1-distill-qwen-1.5b/2 --outtype q4_0 --outfile deepseek-1.5b.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Loading the LLM Model with vLLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code initializes an LLM model using vLLM with specific configurations, such as setting the data type to float16, defining the maximum number of sequences, context length, GPU memory utilization, and enabling remote code trust for downloading the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "# Load the model\n",
    "llm = LLM(model_path,dtype=\"float16\",max_num_seqs=MAX_NUM_SEQS,   # Maximum number of sequences per iteration. Default is 256\n",
    "    max_model_len=MAX_MODEL_LEN, # Model context length\n",
    "    trust_remote_code=True,      # Trust remote code (e.g., from HuggingFace) when downloading the model and tokenizer\n",
    "    tensor_parallel_size=1,      # The number of GPUs to use for distributed execution with tensor parallelism\n",
    "    gpu_memory_utilization=0.95, # The ratio (between 0 and 1) of GPU memory to reserve for the model\n",
    "    seed=2024,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prompt Formatting for Math Problem Solving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function structures the input prompt to enforce strict formatting rules for an LLM solving math problems. It ensures that the model outputs only the final numeric answer inside \\boxed{} without any explanations or intermediate steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(problem_text):\n",
    "    return (\n",
    "        \"Solve the following math problem exactly.\\n\"\n",
    "        \"Return ONLY the final numeric answer inside \\\\boxed{}.\\n\"\n",
    "        \"THIS IS NOT A TRICK QUESTION. DON'T OVERTHINK\"\n",
    "        \"DO NOT include explanations, reasoning, or intermediate steps.\\n\"\n",
    "        \"DO NOT repeat or rephrase the question.\\n\"\n",
    "        \"DO NOT output anything except \\\\boxed{ANSWER}.\\n\"\n",
    "        \"Provide the final numeric answer inside \\\\boxed{} only \\n\\n\"\n",
    "        \"Problem: \" + problem_text\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " max_tokens = MAX_MODEL_LEN\n",
    "if time.time() > cutoff_times[-1]:\n",
    "    print(\"Speedrun\")\n",
    "    max_tokens = 2 * MAX_MODEL_LEN // 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sampling Parameters for Math Problem Solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=1.0,      # Forces deterministic, precise answers\n",
    "    top_p=1.0,           # Consider all likely tokens (not limiting)\n",
    "    top_k=-1,            # No top-k restriction\n",
    "    max_tokens=max_tokens,       # Small max tokens to prevent long reasoning\n",
    "    repetition_penalty=1.1,  # Discourage repeated phrases\n",
    "    ignore_eos=False,    # Let generation stop naturally\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This Will Generate the full Response Using Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def solve_math_problem_full_response(problem_text):\n",
    "#     # Tokenize input\n",
    "#     inputs = tokenizer(problem_text, return_tensors=\"pt\").to(\"cuda\") \n",
    "    \n",
    "#     # Generate response\n",
    "#     with torch.no_grad():\n",
    "#         output = model.generate(**inputs, max_new_tokens=700,  # Increased max_new_tokens\n",
    "#                         eos_token_id=tokenizer.eos_token_id,\n",
    "#                         temperature=0.9,  # Higher temperature\n",
    "#                         top_p=0.85,       # Adjusted top_p\n",
    "#                         num_beams=6,       # More beams\n",
    "#                         repetition_penalty=1.2) # Penalize repetitions     \n",
    "    \n",
    "#     # Decode the response\n",
    "#     response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "#     return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Response Using Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"what is 10+5\"\n",
    "# answer = solve_math_problem_full_response(question)\n",
    "# print(\"Generated Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.  Generating a Math Solution Using vLLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formats the problem prompt to enforce strict answer formatting inside \\boxed{}.\n",
    "Uses vLLM to generate responses based on predefined sampling parameters.\n",
    "Prints the generated output while ensuring no intermediate steps or explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_text = \"What is 10 * 5\"\n",
    "formatted_prompt = format_prompt(problem_text)\n",
    "# Generate response\n",
    "outputs = llm.generate(formatted_prompt,sampling_params)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, \\nGenerated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. To calculate the Modulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_answer(result):\n",
    "    try:\n",
    "        result = int(result)  # Convert to integer\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid input: {result} is not a number\")\n",
    "\n",
    "    # Apply modulo 1000 only if result is greater than 999 or negative\n",
    "    if result > 999 or result < 0:\n",
    "        final_answer = result % 1000\n",
    "        # Ensure positive modulo result\n",
    "        final_answer = final_answer if final_answer >= 0 else final_answer + 1000\n",
    "    else:\n",
    "        final_answer = result\n",
    "\n",
    "    return final_answer  # Returning as an integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_final_answer(1023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Extracting Final Answer from boxed response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_boxed_text(text: str) -> str:\n",
    "    pattern = r'boxed{(.*?)}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "    for match in matches[::-1]:\n",
    "        if match != \"\":\n",
    "            return match\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer= extract_boxed_text(generated_text)\n",
    "print(\"Final Generated Answer:\",final_answer )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_final_answer(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Only Final Boxed Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def extract_boxed_text(text: str) -> str:\n",
    "#     pattern = r'boxed{(.*?)}'\n",
    "#     matches = re.findall(pattern, text)\n",
    "#     if not matches:\n",
    "#         return \"\"\n",
    "#     for match in matches[::-1]:\n",
    "#         if match != \"\":\n",
    "#             return match\n",
    "#     return \"\"\n",
    "\n",
    "# def solve_math_problem(problem_text):\n",
    "#     formatted_prompt = (\n",
    "#     \"Solve the following math problem exactly. Do not approximate. \"\n",
    "#     \"Return ONLY the correct final numeric answer inside \\\\boxed{}. \"\n",
    "#     \"Do NOT include explanations, reasoning, or intermediate steps. \"\n",
    "#     \"If the answer is negative, still use \\\\boxed{}. \"\n",
    "#     \"Ensure the answer is 100% correct before returning. \"\n",
    "#     \"Problem: \" + problem_text\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "#     inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         output = model.generate(**inputs, max_new_tokens=700,  \n",
    "#                         eos_token_id=tokenizer.eos_token_id,\n",
    "#                         temperature=0.9, \n",
    "#                         top_p=0.85,      \n",
    "#                         num_beams=6,     \n",
    "#                         repetition_penalty=1.2)\n",
    "#     response = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "\n",
    "#     return extract_boxed_text(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"what is 10+5\"\n",
    "# answer = solve_math_problem(question)\n",
    "# print(\"Generated Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.  Method that combines prompt formatting, model generation, and extracting the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def solve_math_problem_vllm(llm, problem_text, sampling_params):\n",
    "    \"\"\"\n",
    "    Generates a response using the LLM, extracts the final boxed answer, and returns it.\n",
    "    \n",
    "    Args:\n",
    "        llm: The language model instance.\n",
    "        problem_text (str): The math problem to solve.\n",
    "        sampling_params: Parameters for the model generation.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted final numeric answer from \\boxed{}.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the prompt\n",
    "    formatted_prompt = format_prompt(problem_text)\n",
    "\n",
    "    # Generate response from the model\n",
    "    outputs = llm.generate(formatted_prompt, sampling_params)\n",
    "\n",
    "    # Extract generated text\n",
    "    generated_text = outputs[0].outputs[0].text if outputs else \"\"\n",
    "\n",
    "    # Extract the boxed answer\n",
    "    final_answer = extract_boxed_text(generated_text)\n",
    "    # Modulo \n",
    "    prediction_modulo=compute_final_answer(final_answer)  #Take the Modulo \n",
    "\n",
    "    # Print results\n",
    "    print(f\"Prompt: {formatted_prompt!r}\\n\")\n",
    "    print(f\"Generated text: {generated_text!r}\\n\")\n",
    "    print(f\"Final Generated Answer: {prediction_modulo}\")\n",
    "    return prediction_modulo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"problem\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_text = df_test[\"problem\"][0]\n",
    "final_answer = solve_math_problem_vllm(llm, problem_text, sampling_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to process all rows and generate predictions using Pandas\n",
    "def generate_submission(df_test):\n",
    "    \"\"\"Generate predictions for all rows in df_test and save to submission.csv.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for _, row in df_test.iterrows():  # Iterate through Pandas DataFrame rows\n",
    "        id_ = row[\"id\"]\n",
    "        question = row[\"problem\"]\n",
    "\n",
    "        print(f\"Processing ID: {id_}, Question: {question}\")\n",
    "\n",
    "        # Generate prediction using LLM\n",
    "        prediction = solve_math_problem_vllm(llm, question, sampling_params)\n",
    "\n",
    "        # Append result to list\n",
    "        results.append({\"id\": id_, \"answer\": prediction})\n",
    "\n",
    "    # Convert results to Pandas DataFrame\n",
    "    submission_df = pd.DataFrame(results)\n",
    "\n",
    "    # Save as CSV\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"Submission saved as submission.csv\")\n",
    "\n",
    "# Call the function\n",
    "# generate_submission(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submission(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this function with your inference code.\n",
    "# The function should return a single integer between 0 and 999, inclusive.\n",
    "# Each prediction (except the very first) must be returned within 30 minutes of the question being provided.\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "def predict(id_: pl.DataFrame, question: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
    "    \"\"\"Make a prediction.\"\"\"\n",
    "    # Unpack values\n",
    "    id_ = id_.item(0)\n",
    "    print(\"------\")\n",
    "    print(id_)\n",
    "    \n",
    "    question = question.item(0)\n",
    "    print(question)\n",
    "    # Generate prediction using the model\n",
    "    prediction = solve_math_problem_vllm(llm, question, sampling_params)  # Get boxed answer\n",
    "        \n",
    "    print(\"------\\n\\n\\n\")\n",
    "\n",
    "    print(\"Final Predicted Answer is\",prediction)\n",
    "         \n",
    "    return pl.DataFrame({'id': [id_], 'answer': prediction})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    '/kaggle/input/ai-mathematical-olympiad-progress-prize-2/reference.csv'\n",
    ").drop('answer', axis=1).to_csv('reference.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_server = kaggle_evaluation.aimo_2_inference_server.AIMO2InferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(\n",
    "        (\n",
    "            \"/kaggle/input/ai-mathematical-olympiad-progress-prize-2/test.csv\",\n",
    "            # \"reference.csv\",\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
